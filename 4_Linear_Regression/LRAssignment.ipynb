{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LRAssignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "npYBRa4-hZwc",
        "zwljJM1ti6WC",
        "-oIWnQISjaIQ",
        "t513dvlanvfi",
        "4-AYV8pCsxHN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQ8jBUubwsN"
      },
      "source": [
        "This is the linear regression exercise. First we will implement the algorithm from scratch. Then we will implement it using a widely used library in python- Scikit Learn aka Sklearn. Note that this assignment is relatively lengthy, but nevertheless, do not be discouraged!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uqWV54-cUQP"
      },
      "source": [
        "# Data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_sfF0Bb1EEF"
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQuOZNAcopv"
      },
      "source": [
        "We will be using the Boston housing dataset, a dataset containing details of houses in Boston. We will be using a subset of the actual dataset, which is easily available from sklearn. We will not be performing any exploratory data analysis as it is an extremely simple dataset with no null values and can easily be stored as numpy arrays instead of panda dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0UTtfUv1buf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0fd7be3e-928b-4b3a-be85-f462b3dbaa28"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "X , y = load_boston(return_X_y = True)\n",
        "y = y.reshape(506,1)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13) (506, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHrPG4FRdEcB"
      },
      "source": [
        "Note that m = number of training examples = 506\n",
        "and n = number of features = 13. \n",
        "\n",
        "\n",
        "\n",
        "Next we will split our dataset into a training set and testing set, a must do process in ML. We train our data using only the training test, and test our predictions on the testing set. Here we have divided the data into training and test sets in a 2:1 ratio. For more information check out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rflg4RGkd2QD"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cakhw-6q14U6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oGTcCfzhSm9"
      },
      "source": [
        "# Data Standardization (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVeUNcuyd9LZ"
      },
      "source": [
        "Next we will be performing an operation called data standardization. This basically converts the data roughly into a normal distribution so that gradient descent converges to its global minimum faster. Note that you might've seen this in prob stats :D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ii0Cmmg_pj"
      },
      "source": [
        "\"\"\" Standardization of data. We subtract the mean from each feature and then \n",
        "      divide it by the standard deviation\n",
        "\"\"\"\n",
        "mu =    #ADD CODE HERE to calculate mean OF EACH FEATURE OR COLUMN of the training set. Hint: use numpy\n",
        "sigma = #ADD CODE HERE to calculate standard deviation OF EACH FEATURE of the training set\n",
        "X_train =     #Subtract mean from training data and then divide by standard deviation\n",
        "X_test =      #Subtract mean from testing data and then divide by standard deviation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npYBRa4-hZwc"
      },
      "source": [
        "# Parameter Initialization (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWM1gHrEhewr"
      },
      "source": [
        "Now we will intialize the X matrix and theta vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlI0x_Zu2SjJ"
      },
      "source": [
        "m = y_train.shape[0]\n",
        "\n",
        "def initialize_params(X):\n",
        "  ones =  #Numpy array of ones of size m x 1. This is the bias X0 = 1 which we add \n",
        "  X_new = #Horizontally stack the bias vector to the beginning of the X matrix\n",
        "  theta = #Numpy array of zeros. See theory to check dimensions\n",
        "  return X_new , theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5328gVhSsBZ"
      },
      "source": [
        "X_train, theta = initialize_params(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwljJM1ti6WC"
      },
      "source": [
        "# Cost function (5 marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iUPcP543GI4"
      },
      "source": [
        "def compute_cost(theta, X, y):\n",
        "\n",
        "  J = \n",
        "  return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBi7kb5NXYZ9"
      },
      "source": [
        "compute_cost(theta, X_train, y_train)\n",
        "\n",
        "#If all code is correct you should see a cost of around 307.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oIWnQISjaIQ"
      },
      "source": [
        "# Gradient Descent (5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J2TfFWtjylb"
      },
      "source": [
        "The gradient update rule for each parameter is :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KjK8X5mlE9j"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/880/1*CkcmVCUKmbA-qUn7y8srNQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqrmLimplRjH"
      },
      "source": [
        "However , if we use a for loop, computation is really slow. Therefore we used a vectorized version of the gradient update rule which is :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGugql-6ldDJ"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=17LwD2Tse6w4j4hKCZF33B0ao6yqdzeqD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4yxp2m8bPXH"
      },
      "source": [
        "def gradient_descent(X, y, theta, learning_rate, n_iters):\n",
        "    J_history = np.zeros((n_iters,1))\n",
        "\n",
        "    for i in range(n_iters):\n",
        "      J_history[i] = compute_cost(theta, X, y)\n",
        "      theta = #Vectorized gradient update rule\n",
        "        \n",
        "\n",
        "    return (J_history, theta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t513dvlanvfi"
      },
      "source": [
        "# Fitting the model and predictions (4 marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8BZPSzt4ET7"
      },
      "source": [
        "(J_history, theta) = #Call the gradient descent function with the training data, with a learning rate of 0.1, for 50 iterations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZz0JhOK5zQq"
      },
      "source": [
        "print(J_history) #If all code is correct, the last value of the cost should be around 11.795"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEo1i_nR9ax3"
      },
      "source": [
        "plt.plot(range(len(J_history)), J_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Cost')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDle8PVUqc4a"
      },
      "source": [
        "Observe that the cost decreases rapidly at first then slows down.After the 30th iteration, there isn't much change. So training for too many iterations after 30 iterations doesn't make sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAUUCbdnjy-j"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "X_test = np.hstack((np.ones((X_test.shape[0],1)) , X_test))\n",
        "y_pred = X_test @ theta\n",
        "print(0.5 * mean_squared_error(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnZ-12KHsQKH"
      },
      "source": [
        "Great! Our loss on the test set is low as well. This means we have trained a pretty good model. Note that many times, the training loss will be very low while the test loss be relatively high. This is a problem known as ***overfitting*** that we will discuss later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1B6fpJnsl6m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-AYV8pCsxHN"
      },
      "source": [
        "# Linear Regression Using Scikit Learn(5 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnOj1mh4xubK"
      },
      "source": [
        "Phew! All the code above can be written in just a couple of lines using scikit learn! For this section, we leave it up to you to google the code and write it on your own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_PtOosrtKOX"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyw6ZuKWtMvM"
      },
      "source": [
        "linear_model =  #Create a linear regression object off the LinearRegression class\n",
        "\n",
        "#Now use the fit() method of the model on the training data below\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZdsyPEEtabw"
      },
      "source": [
        "#Use the predict function to get the predictions of the test and training data\n",
        "\n",
        "yPred_train = \n",
        "yPred_test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgrv7DGuv7Av"
      },
      "source": [
        "print(\"Training loss = \" + str(0.5 * mean_squared_error(yPred_train, y_train)))\n",
        "print(\"Test loss = \" + str(0.5 * mean_squared_error(yPred_test, y_test)))\n",
        "\n",
        "#If code is right you should see training loss of around 11.49 and test loss of 11.19"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fooZ-MpwGJL"
      },
      "source": [
        "Awesome! Our model from scratch performs almost as good as the one from scikit learn. Remember that we use scikit learn extensively for ML algorithms, however, we have implemented it from scratch for the sake of understanding, which is crucial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drZsxlCBwtlc"
      },
      "source": [
        "Congratulations on completing your first machine learning algorithm!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUF6PNLpww5k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}